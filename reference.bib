@article{Johnson:9MAvkbgy,
  title={Accelerating Stochastic Gradient Descent using Predictive Variance Reduction},
  author={Johnson, R and Zhang, T},
  journal={Advances in Neural Information Processing Systems},
  pages={315--323},
  year={2013},
}

@article{Richtarik:2013te,
author = {Richt{\'a}rik, Peter},
title = {{Semi-Stochastic Gradient Descent Methods}},
journal = {arXiv.org},
year = {2013},
eprint = {1312.1666v2},
eprinttype = {arxiv},
eprintclass = {stat.ML},
month = Dec,
}

@article{Liu:2015bx,
author = {Liu, Jie and Richt{\'a}rik, Peter},
title = {{Mini-Batch Semi-Stochastic Gradient Descent in the Proximal Setting}},
journal = {arXiv.org},
year = {2015},
eprint = {1504.04407v2},
eprinttype = {arxiv},
eprintclass = {cs.LG},
pages = {242--255},
month = apr
}
@article{Zhang2013Linear,
author = {Zhang, L and Mahdavi, M and Jin, R},
title = {{Linear convergence with condition number independent access of full gradients}},
journal = {Advances in Neural Information Processing Systems},
year = {2013},
pages = {980--988}
}

@article{Li:2016vh,
author = {Li, Xingguo and Zhao, Tuo and Arora, Raman and Liu, Han and Haupt, Jarvis},
title = {{Stochastic Variance Reduced Optimization for Nonconvex Sparse Learning}},
journal = {arXiv.org},
year = {2016},
eprint = {1605.02711v1},
eprinttype = {arxiv},
eprintclass = {cs.LG},
month = may,
annote = {Accepted by International Conference on Machine Learning (ICML) 2016}
}
@article{Xiao:2014vw,
author = {Xiao, Lin and Zhang, Tong},
title = {{A Proximal Stochastic Gradient Method with Progressive Variance Reduction}},
journal = {arXiv.org},
year = {2014},
eprint = {1403.4699v1},
eprinttype = {arxiv},
eprintclass = {math.OC},
month = mar
}
@article{Allenzhu2016Katyusha,
author = {Allenzhu, Zeyuan},
title = {{Katyusha: Accelerated Variance Reduction for Faster SGD}},
journal = {arXiv.org},
year = {2016}
}




@article{Allen2015Improved,
author = {Allen-Zhu, Zeyuan and Yuan, Yang},
title = {{Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives}},
journal = {Mathematics},
year = {2015}
}

@article{Defazio:2014vu,
author = {Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
title = {{SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives}},
journal = {arXiv.org},
year = {2014},
eprint = {1407.0202v3},
eprinttype = {arxiv},
eprintclass = {cs.LG},
month = jul,
annote = {Advances In Neural Information Processing Systems, Nov 2014, Montreal, Canada}
}

@article{Schmidt:2013ui,
author = {Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
title = {{Minimizing Finite Sums with the Stochastic Average Gradient}},
journal = {arXiv.org},
year = {2013},
eprint = {1309.2388v2},
eprinttype = {arxiv},
eprintclass = {math.OC},
month = Sep,
annote = {Revision from January 2015 submission. Major changes: updated literature follow and discussion of subsequent work, additional Lemma showing the validity of one of the formulas, somewhat simplified presentation of Lyapunov bound, included code needed for checking proofs rather than the polynomials generated by the code, added error regions to the numerical experiments}
}


